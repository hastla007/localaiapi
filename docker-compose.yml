version: '3.8'

services:
  ai-api:
    build:
      context: .
      dockerfile: Dockerfile_nightly
    container_name: ai-api-local
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
      - ./outputs:/app/outputs
      - ./cache:/app/cache
      - ./templates:/app/templates
    environment:
      - HF_HOME=/app/cache
      - TRANSFORMERS_CACHE=/app/cache
      - CUDA_VISIBLE_DEVICES=0
      - MAX_LOADED_MODELS=2
      - MODEL_TIMEOUT=300
      - HF_TOKEN=here_comes_api_key  # ‚Üê ADD THIS LINE
      # Blackwell (RTX 5070 Ti) optimizations
      - TORCH_CUDA_ARCH_LIST=8.0;8.6;8.9;9.0;12.0
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128
      - CUDA_LAUNCH_BLOCKING=0
      - TORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    shm_size: '16gb'  # Increase shared memory for larger models
