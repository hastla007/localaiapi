# Stable PyTorch 2.4.x on CUDA 12.4 (cuDNN 9), optimized for RTX 50-series
# Use the official PyTorch image that already bundles CUDA 12.4 runtime
FROM pytorch/pytorch:2.4.0-cuda12.4-cudnn9-runtime

# System deps (minimal). Add dev tools only if you need to build native libs.
RUN apt-get update && apt-get install -y --no-install-recommends \
    git wget curl ca-certificates tini \
    && rm -rf /var/lib/apt/lists/*

# Environment for reliability and HF cache
ENV DEBIAN_FRONTEND=noninteractive \
    PIP_NO_CACHE_DIR=1 \
    HF_HOME=/root/.cache/huggingface \
    CUDA_HOME=/usr/local/cuda \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Keep pip modern
RUN python -m pip install --upgrade pip

# Ensure we have the stable CUDA 12.4 wheels (even though the base image includes torch)
# This guards against accidental downgrades/CPU-only installs from later layers.
RUN pip install --upgrade --index-url https://download.pytorch.org/whl/cu124 \
    torch torchvision torchaudio

# (Optional) Common ML utilities â€” comment out if not needed
# RUN pip install --no-cache-dir \
#     numpy scipy pandas tqdm matplotlib \
#     transformers accelerate datasets \
#     lightning

# Quick runtime verification
RUN python - <<'PY'\n\
import torch\n\
print('Torch:', torch.__version__)\n\
print('CUDA runtime:', getattr(torch.version, 'cuda', 'N/A'))\n\
print('CUDA available:', torch.cuda.is_available())\n\
print('Device:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A')\n\
PY

# Default entry
ENTRYPOINT [\"/usr/bin/tini\", \"--\"]\n\
CMD [\"python\"]\n